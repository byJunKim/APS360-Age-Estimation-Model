{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Accuracy_Module.py\n",
    "%run DataLoading.py\n",
    "%run load_and_organize_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install p7zip-full\n",
    "!p7zip -d UTKFace.tar.gz\n",
    "!tar -xvf UTKFace.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path = 'UTKFace/'\n",
    "out_path = 'Data/'\n",
    "\n",
    "count = organize_files(in_path, out_path, 1, 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = load_dataset(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_data:\n",
    "    data, label =  data\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(pretrained = False)\n",
    "vgg16 = nn.Sequential(vgg16.features, vgg16.avgpool)\n",
    "\n",
    "pre_trained = torch.load(\"vgg_face_dag.pth\")\n",
    "new = list(pre_trained.items())\n",
    "vgg16_state = vgg16.state_dict()\n",
    "count = 0\n",
    "for key, value in vgg16_state.items():\n",
    "    layer_name, weights = new[count]      \n",
    "    vgg16_state[key] = weights\n",
    "    count += 1\n",
    "\n",
    "vgg16.load_state_dict(vgg16_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (1): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16.cuda()\n",
    "vgg16.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_regression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG_regression, self).__init__()\n",
    "        self.name = \"vgg_regression\"\n",
    "        self.fc1 = nn.Linear(25088, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 1024)\n",
    "        self.fc4 = nn.Linear(1024, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.cuda()\n",
    "        x = vgg16(x)\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        x = x.squeeze(1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_eval, net, criterion, batch_size =32):\n",
    "    total_epoch = 0\n",
    "    total_loss = 0\n",
    "    for inputs, labels in data_eval:\n",
    "        outputs = net(inputs.cuda())\n",
    "        loss = criterion(outputs.cuda(), labels.float().cuda())\n",
    "        total_loss += loss.item()\n",
    "        total_epoch += len(labels)\n",
    "    return float(total_loss)/(total_epoch)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(train_data, valid_data, net, pretrained = False, checkpoint = None, batch_size=32, learning_rate=5e-05, num_epochs=30):\n",
    "    \n",
    "    \n",
    "    ########################################################################\n",
    "    # Fixed PyTorch random seed for reproducible result\n",
    "    from torch.autograd import Variable\n",
    "    torch.manual_seed(1000)\n",
    "    ########################################################################\n",
    "    # Obtain the PyTorch data loader objects to load batches of the datasets\n",
    "    #train_loader, val_loader, test_loader = load_dataset(batch_size)\n",
    "    ########################################################################\n",
    "    # Define the Loss function and optimizer\n",
    "    # The loss function will be \n",
    "    # Optimizer will be SGD with Momentum.\n",
    "    \n",
    "    criterion = nn.MSELoss().cuda()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    \n",
    "    pretrained_epoch = 0\n",
    "    train_losses, val_losses, train_acc, val_acc = [], [], [], []\n",
    "    if pretrained == True:\n",
    "        net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        pretrained_epoch = checkpoint['epoch']\n",
    "        train_losses = checkpoint['train_loss']\n",
    "        val_losses = checkpoint['valid_loss']\n",
    "        net.train()\n",
    "        print(\"resuming training after epoch: \", pretrained_epoch)\n",
    "    \n",
    "    # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    ########################################################################\n",
    "    # Train the network\n",
    "    # Loop over the data iterator and sample a new batch of training data\n",
    "    # Get the output from the network, and optimize our loss function.\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        total_epoch = 0\n",
    "        total_train_loss = 0\n",
    "        #random.shuffle(train_data)\n",
    "        for inputs, labels in train_data:\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass, backward pass, and optimize\n",
    "            outputs = net(Variable(inputs.cuda()))\n",
    "            loss = criterion(outputs.cuda(), labels.float().cuda())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            total_epoch += len(labels) #adding batch size\n",
    "            \n",
    "        # save the current training information\n",
    "        train_losses.append(float(total_train_loss)/total_epoch)            # compute *average* loss\n",
    "        val_losses.append(evaluate(valid_data, net, criterion, batch_size = batch_size))\n",
    "        #train_acc.append(get_accuracy(net, train_data, batch_size = batch_size)) # compute training accuracy \n",
    "        #val_acc.append(get_accuracy(net, valid_data, batch_size = batch_size))  # compute validation accuracy\n",
    "            \n",
    "        #print(\"Epoch: {}, Training Loss: {:.3f}, Validation Loss: {:.3f}, Training Accuracy: {:.3f}, Validation Accuracy: {:.3f}\".format(epoch+pretrained_epoch+1, train_losses[-1], val_losses[-1], train_acc[-1],val_acc[-1]))\n",
    "        print(\"Epoch: {}, Training Loss: {:.3f}, Validation Loss: {:.3f}\".format(epoch+pretrained_epoch+1, train_losses[-1], val_losses[-1]))\n",
    "        # Save the current model (checkpoint) to a file\n",
    "        #torch.save({ 'epoch': epoch, 'model_state_dict': net.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'train_loss': train_losses, 'train_accuracy': train_acc, 'valid_loss': val_losses, 'valid_accuracy': val_acc}, \n",
    "        #           \"{}_features_bs{}_lr{}\".format(pretrained, batch_size, learning_rate))\n",
    "        \n",
    "        torch.save({ 'epoch': epoch+pretrained_epoch+1, 'model_state_dict': net.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'train_loss': train_losses, 'valid_loss': val_losses}, \n",
    "                   \"VGGfeatures_bs{}_lr{}\".format(pretrained, batch_size, learning_rate))\n",
    "        \n",
    "        #saving it in the google cloud storage\n",
    "        client = storage.Client()\n",
    "        bucket = client.get_bucket(\"aps360team12\")\n",
    "        blob_name = \"VGGfeatures_bs{}_lr{}_epoch{}\".format(pretrained, batch_size, learning_rate, epoch+pretrained_epoch)\n",
    "        blob = bucket.blob(blob_name)\n",
    "\n",
    "        source_file_name = \"VGGfeatures_bs{}_lr{}\".format(pretrained, batch_size, learning_rate)\n",
    "        blob.upload_from_filename(source_file_name)\n",
    "        print(\"File uploaded to {}.\".format(bucket.name))\n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "    # plotting\n",
    "    plt.title(\"Loss curves w/ lr={}, batch size = {}\".format(learning_rate, batch_size))\n",
    "    plt.axis([0, num_epochs, 0, max(train_losses[0], val_losses[0])])\n",
    "    plt.plot(train_losses, label=\"Train loss\")\n",
    "    plt.plot(val_losses, label=\"Validation loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Acc curves w/ lr={}, batch size = {}\".format(learning_rate, batch_size))\n",
    "    plt.axis([0, num_epochs, 0, min(train_acc[0], val_acc[0])])\n",
    "    plt.plot(train_acc, label=\"Train\")\n",
    "    plt.plot(val_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Training Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
    "    print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))\n",
    "\n",
    "    print('Finished Training')\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data, batch_size = 32):\n",
    "    c=0\n",
    "    mean = 0.0\n",
    "    for imgs, labels in data:\n",
    "        mean += labels.sum()\n",
    "        c+=batch_size\n",
    "    mean = (mean/c)\n",
    "    \n",
    "    #print(mean)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    count = 0 \n",
    "    ss_reg = 0\n",
    "    ss_total = 0\n",
    "    \n",
    "    for imgs, labels in data:\n",
    "        labels = labels.float()\n",
    "        output = model(imgs) # We don't need to run F.softmax\n",
    "        # print(output)\n",
    "        # pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        # correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        output = torch.round(output)\n",
    "        output = output.float()\n",
    "        output = output.cpu().detach().numpy()\n",
    "        output = torch.tensor(output)\n",
    "        #correct += np.isclose(output.detach().numpy(), labels, 0.05).sum()\n",
    "        total += imgs.shape[0]\n",
    "        count+=1\n",
    "        ss_reg += ((labels-output)**2).sum()\n",
    "        ss_total += ((labels-mean)**2).sum()\n",
    "    return 1-ss_reg/ss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG_regression(\n",
       "  (fc1): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (fc3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (fc4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.2)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_regression = VGG_regression()\n",
    "vgg_regression.cuda()\n",
    "vgg_regression2 = VGG_regression()\n",
    "vgg_regression2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_net(train_data, valid_data, vgg_regression, batch_size = 128, num_epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client()\n",
    "bucket_name = \"aps360team12\"\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "blob_name = \"VGG_features_bs64_lr5e-05_epoch6\"\n",
    "blob = bucket.get_blob(blob_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.download_to_filename('pretrained_epoch6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_data = torch.load('pretrained_epoch6')\n",
    "trained_data['epoch'] = trained_data['epoch'] + 1 #you dont need to do this anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_loss = trained_data['train_loss']\n",
    "val_loss = trained_data['valid_loss']\n",
    "vgg_regression.load_state_dict(trained_data['model_state_dict'])\n",
    "vgg_regression.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate(test_data, vgg_regression, nn.MSELoss().cuda(), batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "for img, label in test_data:\n",
    "    #img = img / 2 + 0.5\n",
    "    #plt.subplot(3, 5, k+1)\n",
    "    #plt.axis('off')\n",
    "    pred = vgg_regression(img)\n",
    "    for count, i in enumerate(img, 0):\n",
    "        print(count)\n",
    "        i = np.transpose(i, [1,2,0])\n",
    "        plt.imshow(i)\n",
    "        plt.show()\n",
    "        print(\"Actual Age: \", label[count], \" Prediction: \", pred[count])\n",
    "        k += 1\n",
    "    #print(label, vgg_regression(img))\n",
    "        if k > 10:\n",
    "            break\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resuming training after epoch:  6\n",
      "Epoch: 7, Training Loss: 0.650, Validation Loss: 1.003\n",
      "File uploaded to aps360team12.\n",
      "Epoch: 8, Training Loss: 0.597, Validation Loss: 1.021\n",
      "File uploaded to aps360team12.\n",
      "Epoch: 9, Training Loss: 0.518, Validation Loss: 1.033\n",
      "File uploaded to aps360team12.\n",
      "Epoch: 10, Training Loss: 0.439, Validation Loss: 1.012\n",
      "File uploaded to aps360team12.\n",
      "Epoch: 11, Training Loss: 0.382, Validation Loss: 1.037\n",
      "File uploaded to aps360team12.\n",
      "Epoch: 12, Training Loss: 0.325, Validation Loss: 1.041\n",
      "File uploaded to aps360team12.\n",
      "Epoch: 13, Training Loss: 0.262, Validation Loss: 1.161\n",
      "File uploaded to aps360team12.\n",
      "Epoch: 14, Training Loss: 0.225, Validation Loss: 1.086\n",
      "File uploaded to aps360team12.\n",
      "Epoch: 15, Training Loss: 0.190, Validation Loss: 1.094\n",
      "File uploaded to aps360team12.\n",
      "Epoch: 16, Training Loss: 0.175, Validation Loss: 1.056\n",
      "File uploaded to aps360team12.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-78ce41d345b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvgg_regression2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrained_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-6a0bc7d331be>\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(train_data, valid_data, net, pretrained, checkpoint, batch_size, learning_rate, num_epochs)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# Forward pass, backward pass, and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_net(train_data, valid_data, vgg_regression2, pretrained = True, checkpoint = trained_data, batch_size = 64, num_epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client()\n",
    "bucket = client.get_bucket(\"aps360team12\")\n",
    "blob_name = \"VGG_features_bs32_lr5e-05\"\n",
    "blob = bucket.blob(blob_name)\n",
    "\n",
    "source_file_name = \"{}_features_bs{}_lr{}\".format(\"VGG\", 32, 5e-05)\n",
    "blob.upload_from_filename(source_file_name)\n",
    "\n",
    "print(\"File uploaded to {}.\".format(bucket.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(test_data, vgg_regression, nn.MSELoss().cuda(), batch_size = 64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
